{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Pull from API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Library Imports\n",
    "\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import tensorflow\n",
    "import warnings\n",
    "from keras import optimizers\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.models import Sequential\n",
    "from keras.models import load_model\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the dataset\n",
    "\n",
    "dataset = pd.read_csv('data/GOOG.csv')\n",
    "dataset = dataset[['Open', 'High', 'Low', 'Close']]\n",
    "\n",
    "# Feature Engineering \n",
    "\n",
    "dataset['H-L'] = dataset['High'] - dataset['Low']\n",
    "dataset['O-C'] = dataset['Close'] - dataset['Open']\n",
    "dataset['3day MA'] = dataset['Close'].shift(1).rolling(window = 3).mean()\n",
    "dataset['10day MA'] = dataset['Close'].shift(1).rolling(window = 10).mean()\n",
    "dataset['30day MA'] = dataset['Close'].shift(1).rolling(window = 30).mean()\n",
    "dataset['Std_dev']= dataset['Close'].rolling(5).std()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Identify price rises. Using a binary variable, 1, to indicate when\n",
    "the closing price of the next day is greater than the closing price of prior day.\n",
    "'''\n",
    "\n",
    "dataset['Price_Rise'] = np.where(dataset['Close'].shift(-1) > dataset['Close'], 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>H-L</th>\n",
       "      <th>O-C</th>\n",
       "      <th>3day MA</th>\n",
       "      <th>10day MA</th>\n",
       "      <th>30day MA</th>\n",
       "      <th>Std_dev</th>\n",
       "      <th>Price_Rise</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>96.438522</td>\n",
       "      <td>99.546875</td>\n",
       "      <td>96.179497</td>\n",
       "      <td>97.250481</td>\n",
       "      <td>3.367378</td>\n",
       "      <td>0.811959</td>\n",
       "      <td>94.374593</td>\n",
       "      <td>98.017605</td>\n",
       "      <td>96.714490</td>\n",
       "      <td>1.657058</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>96.986473</td>\n",
       "      <td>99.292824</td>\n",
       "      <td>96.787216</td>\n",
       "      <td>98.834541</td>\n",
       "      <td>2.505608</td>\n",
       "      <td>1.848068</td>\n",
       "      <td>95.578415</td>\n",
       "      <td>98.183483</td>\n",
       "      <td>96.590289</td>\n",
       "      <td>2.349867</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>98.545624</td>\n",
       "      <td>99.502045</td>\n",
       "      <td>98.037529</td>\n",
       "      <td>98.580498</td>\n",
       "      <td>1.464516</td>\n",
       "      <td>0.034874</td>\n",
       "      <td>97.406563</td>\n",
       "      <td>97.807393</td>\n",
       "      <td>96.655212</td>\n",
       "      <td>2.227804</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>98.884354</td>\n",
       "      <td>99.048744</td>\n",
       "      <td>97.962814</td>\n",
       "      <td>98.605400</td>\n",
       "      <td>1.085930</td>\n",
       "      <td>-0.278954</td>\n",
       "      <td>98.221840</td>\n",
       "      <td>97.161813</td>\n",
       "      <td>96.728106</td>\n",
       "      <td>1.158720</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>97.883110</td>\n",
       "      <td>99.078629</td>\n",
       "      <td>94.839516</td>\n",
       "      <td>95.327690</td>\n",
       "      <td>4.239113</td>\n",
       "      <td>-2.555420</td>\n",
       "      <td>98.673480</td>\n",
       "      <td>96.842509</td>\n",
       "      <td>96.884187</td>\n",
       "      <td>1.475651</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Open       High        Low      Close       H-L       O-C    3day MA  \\\n",
       "30  96.438522  99.546875  96.179497  97.250481  3.367378  0.811959  94.374593   \n",
       "31  96.986473  99.292824  96.787216  98.834541  2.505608  1.848068  95.578415   \n",
       "32  98.545624  99.502045  98.037529  98.580498  1.464516  0.034874  97.406563   \n",
       "33  98.884354  99.048744  97.962814  98.605400  1.085930 -0.278954  98.221840   \n",
       "34  97.883110  99.078629  94.839516  95.327690  4.239113 -2.555420  98.673480   \n",
       "\n",
       "     10day MA   30day MA   Std_dev  Price_Rise  \n",
       "30  98.017605  96.714490  1.657058           1  \n",
       "31  98.183483  96.590289  2.349867           0  \n",
       "32  97.807393  96.655212  2.227804           1  \n",
       "33  97.161813  96.728106  1.158720           0  \n",
       "34  96.842509  96.884187  1.475651           1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop NAN values\n",
    "\n",
    "dataset = dataset.dropna()\n",
    "\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperate target variable and drop target variable from df\n",
    "\n",
    "df = dataset.drop('Price_Rise', axis=1)\n",
    "y = dataset['Price_Rise']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the Dataset\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df, y, test_size=0.20, \n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.517904\n",
       "0    0.482096\n",
       "Name: Price_Rise, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Establish a Baseline & Domain Insight:\n",
    "\n",
    "Objective: to accomplish a better accuracy % then our baseline of 51.7 pct. \n",
    "Why such a low baseline? Casino's make their money with 51%/49% odds. Any rate\n",
    "better then a 51% accuracy rate should be considered as a success.\n",
    "'''\n",
    "\n",
    "y_train.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Feature Scaling and Standardizing:\n",
    "\n",
    "This ensures that there is no bias while training the model due to the \n",
    "different scales of all input features. If this is not done the neural \n",
    "network might get confused and give a higher weight to those features \n",
    "which have a higher average value than others.\n",
    "'''\n",
    "\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nNN Notes:\\n\\nUnits: This defines the number of nodes or neurons in that particular layer. \\nWe have set this value to 128, meaning there will be 128 neurons in our hidden layer.\\n\\nKernel_initializer: This defines the starting values for the weights of the \\ndifferent neurons in the hidden layer. We have defined this to be ‘uniform’, \\nwhich means that the weights will be initialized with values from a uniform distribution.\\n\\nActivation: This is the activation function for the neurons in the particular hidden layer. \\nHere we define the function as the rectified Linear Unit function or ‘relu’.\\n\\nInputs: This defines the number of inputs to the hidden layer, we have defined this \\nvalue to be equal to the number of columns of our input feature dataframe. \\nThis argument will not be required in the subsequent layers, as the model will know \\nhow many outputs the previous layer produced.\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "NN Notes:\n",
    "\n",
    "Units: This defines the number of nodes or neurons in that particular layer. \n",
    "We have set this value to 128, meaning there will be 128 neurons in our hidden layer.\n",
    "\n",
    "Kernel_initializer: This defines the starting values for the weights of the \n",
    "different neurons in the hidden layer. We have defined this to be ‘uniform’, \n",
    "which means that the weights will be initialized with values from a uniform distribution.\n",
    "\n",
    "Activation: This is the activation function for the neurons in the particular hidden layer. \n",
    "Here we define the function as the rectified Linear Unit function or ‘relu’.\n",
    "\n",
    "Inputs: This defines the number of inputs to the hidden layer, we have defined this \n",
    "value to be equal to the number of columns of our input feature dataframe. \n",
    "This argument will not be required in the subsequent layers, as the model will know \n",
    "how many outputs the previous layer produced.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Variables for NN - Adjust as neccessary. \n",
    "'''\n",
    "epochs = 100\n",
    "batch_size = 10\n",
    "inputs = X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "A breakdown of the NN Architecture\n",
    "'''\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Two Hidden Layers\n",
    "\n",
    "model.add(Dense(128, activation='relu', input_shape=(inputs,)))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "\n",
    "# Output Layer\n",
    "\n",
    "model.add(Dense(1, kernel_initializer='uniform', activation='sigmoid'))\n",
    "\n",
    "# Compile the Model\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='mean_squared_error',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Builds a function for our model, incorporates tensorboard for visual analysis\n",
    "'''\n",
    "\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, activation='relu', input_shape=(inputs,)))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='uniform', activation='sigmoid'))\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='mean_squared_error',\n",
    "                  metrics=['acc'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "3072/3072 [==============================] - 0s 126us/step - loss: 0.2509 - acc: 0.4993\n",
      "Epoch 2/100\n",
      "3072/3072 [==============================] - 0s 95us/step - loss: 0.2497 - acc: 0.5179\n",
      "Epoch 3/100\n",
      "3072/3072 [==============================] - 0s 112us/step - loss: 0.2498 - acc: 0.5117\n",
      "Epoch 4/100\n",
      "3072/3072 [==============================] - 0s 118us/step - loss: 0.2496 - acc: 0.5238\n",
      "Epoch 5/100\n",
      "3072/3072 [==============================] - 0s 125us/step - loss: 0.2496 - acc: 0.5244\n",
      "Epoch 6/100\n",
      "3072/3072 [==============================] - 1s 174us/step - loss: 0.2491 - acc: 0.5199\n",
      "Epoch 7/100\n",
      "3072/3072 [==============================] - 0s 137us/step - loss: 0.2493 - acc: 0.5225\n",
      "Epoch 8/100\n",
      "3072/3072 [==============================] - 0s 142us/step - loss: 0.2492 - acc: 0.5277\n",
      "Epoch 9/100\n",
      "3072/3072 [==============================] - 0s 136us/step - loss: 0.2488 - acc: 0.5254\n",
      "Epoch 10/100\n",
      "3072/3072 [==============================] - 0s 124us/step - loss: 0.2484 - acc: 0.5277\n",
      "Epoch 11/100\n",
      "3072/3072 [==============================] - 0s 124us/step - loss: 0.2487 - acc: 0.5286\n",
      "Epoch 12/100\n",
      "3072/3072 [==============================] - 0s 113us/step - loss: 0.2485 - acc: 0.5280\n",
      "Epoch 13/100\n",
      "3072/3072 [==============================] - 0s 109us/step - loss: 0.2484 - acc: 0.5267\n",
      "Epoch 14/100\n",
      "3072/3072 [==============================] - 0s 105us/step - loss: 0.2482 - acc: 0.5319\n",
      "Epoch 15/100\n",
      "3072/3072 [==============================] - 0s 113us/step - loss: 0.2479 - acc: 0.5358\n",
      "Epoch 16/100\n",
      "3072/3072 [==============================] - 0s 110us/step - loss: 0.2477 - acc: 0.5335\n",
      "Epoch 17/100\n",
      "3072/3072 [==============================] - 0s 116us/step - loss: 0.2477 - acc: 0.5365\n",
      "Epoch 18/100\n",
      "3072/3072 [==============================] - 0s 122us/step - loss: 0.2472 - acc: 0.5423\n",
      "Epoch 19/100\n",
      "3072/3072 [==============================] - 0s 114us/step - loss: 0.2474 - acc: 0.5365\n",
      "Epoch 20/100\n",
      "3072/3072 [==============================] - 0s 112us/step - loss: 0.2472 - acc: 0.5358\n",
      "Epoch 21/100\n",
      "3072/3072 [==============================] - 0s 115us/step - loss: 0.2463 - acc: 0.5397\n",
      "Epoch 22/100\n",
      "3072/3072 [==============================] - 0s 135us/step - loss: 0.2471 - acc: 0.5436\n",
      "Epoch 23/100\n",
      "3072/3072 [==============================] - 0s 119us/step - loss: 0.2459 - acc: 0.5485\n",
      "Epoch 24/100\n",
      "3072/3072 [==============================] - 0s 119us/step - loss: 0.2464 - acc: 0.5449\n",
      "Epoch 25/100\n",
      "3072/3072 [==============================] - 0s 120us/step - loss: 0.2461 - acc: 0.5469\n",
      "Epoch 26/100\n",
      "3072/3072 [==============================] - 0s 106us/step - loss: 0.2462 - acc: 0.5540\n",
      "Epoch 27/100\n",
      "3072/3072 [==============================] - 0s 111us/step - loss: 0.2455 - acc: 0.5511\n",
      "Epoch 28/100\n",
      "3072/3072 [==============================] - 0s 120us/step - loss: 0.2455 - acc: 0.5531\n",
      "Epoch 29/100\n",
      "3072/3072 [==============================] - 0s 112us/step - loss: 0.2447 - acc: 0.5531\n",
      "Epoch 30/100\n",
      "3072/3072 [==============================] - 0s 110us/step - loss: 0.2446 - acc: 0.5540\n",
      "Epoch 31/100\n",
      "3072/3072 [==============================] - 0s 112us/step - loss: 0.2453 - acc: 0.5501\n",
      "Epoch 32/100\n",
      "3072/3072 [==============================] - 0s 112us/step - loss: 0.2447 - acc: 0.5492\n",
      "Epoch 33/100\n",
      "3072/3072 [==============================] - 0s 102us/step - loss: 0.2441 - acc: 0.5592\n",
      "Epoch 34/100\n",
      "3072/3072 [==============================] - 0s 104us/step - loss: 0.2444 - acc: 0.5501\n",
      "Epoch 35/100\n",
      "3072/3072 [==============================] - 0s 100us/step - loss: 0.2438 - acc: 0.5527\n",
      "Epoch 36/100\n",
      "3072/3072 [==============================] - 0s 99us/step - loss: 0.2432 - acc: 0.5560\n",
      "Epoch 37/100\n",
      "3072/3072 [==============================] - 0s 100us/step - loss: 0.2434 - acc: 0.5592\n",
      "Epoch 38/100\n",
      "3072/3072 [==============================] - 0s 104us/step - loss: 0.2429 - acc: 0.5618\n",
      "Epoch 39/100\n",
      "3072/3072 [==============================] - 0s 96us/step - loss: 0.2426 - acc: 0.5612\n",
      "Epoch 40/100\n",
      "3072/3072 [==============================] - 0s 101us/step - loss: 0.2426 - acc: 0.5592\n",
      "Epoch 41/100\n",
      "3072/3072 [==============================] - 0s 99us/step - loss: 0.2429 - acc: 0.5602\n",
      "Epoch 42/100\n",
      "3072/3072 [==============================] - 0s 111us/step - loss: 0.2413 - acc: 0.5638\n",
      "Epoch 43/100\n",
      "3072/3072 [==============================] - 0s 116us/step - loss: 0.2413 - acc: 0.5687\n",
      "Epoch 44/100\n",
      "3072/3072 [==============================] - 0s 110us/step - loss: 0.2413 - acc: 0.5648\n",
      "Epoch 45/100\n",
      "3072/3072 [==============================] - 0s 101us/step - loss: 0.2404 - acc: 0.5693\n",
      "Epoch 46/100\n",
      "3072/3072 [==============================] - 0s 128us/step - loss: 0.2406 - acc: 0.5713\n",
      "Epoch 47/100\n",
      "3072/3072 [==============================] - 0s 136us/step - loss: 0.2406 - acc: 0.5635\n",
      "Epoch 48/100\n",
      "3072/3072 [==============================] - 0s 122us/step - loss: 0.2401 - acc: 0.5684\n",
      "Epoch 49/100\n",
      "3072/3072 [==============================] - 0s 111us/step - loss: 0.2401 - acc: 0.5641\n",
      "Epoch 50/100\n",
      "3072/3072 [==============================] - 1s 164us/step - loss: 0.2397 - acc: 0.5710\n",
      "Epoch 51/100\n",
      "3072/3072 [==============================] - 0s 127us/step - loss: 0.2390 - acc: 0.5752\n",
      "Epoch 52/100\n",
      "3072/3072 [==============================] - 0s 109us/step - loss: 0.2396 - acc: 0.5713\n",
      "Epoch 53/100\n",
      "3072/3072 [==============================] - 0s 134us/step - loss: 0.2390 - acc: 0.5716\n",
      "Epoch 54/100\n",
      "3072/3072 [==============================] - 1s 168us/step - loss: 0.2384 - acc: 0.5742\n",
      "Epoch 55/100\n",
      "3072/3072 [==============================] - 0s 126us/step - loss: 0.2376 - acc: 0.5723\n",
      "Epoch 56/100\n",
      "3072/3072 [==============================] - 1s 175us/step - loss: 0.2387 - acc: 0.5755\n",
      "Epoch 57/100\n",
      "3072/3072 [==============================] - 0s 132us/step - loss: 0.2376 - acc: 0.5719\n",
      "Epoch 58/100\n",
      "3072/3072 [==============================] - 0s 120us/step - loss: 0.2370 - acc: 0.5716\n",
      "Epoch 59/100\n",
      "3072/3072 [==============================] - 0s 122us/step - loss: 0.2369 - acc: 0.5755\n",
      "Epoch 60/100\n",
      "3072/3072 [==============================] - 0s 113us/step - loss: 0.2371 - acc: 0.5710\n",
      "Epoch 61/100\n",
      "3072/3072 [==============================] - 0s 118us/step - loss: 0.2370 - acc: 0.5700\n",
      "Epoch 62/100\n",
      "3072/3072 [==============================] - 0s 121us/step - loss: 0.2368 - acc: 0.5732\n",
      "Epoch 63/100\n",
      "3072/3072 [==============================] - 0s 126us/step - loss: 0.2358 - acc: 0.5752\n",
      "Epoch 64/100\n",
      "3072/3072 [==============================] - 0s 119us/step - loss: 0.2360 - acc: 0.5765\n",
      "Epoch 65/100\n",
      "3072/3072 [==============================] - 0s 129us/step - loss: 0.2359 - acc: 0.5778\n",
      "Epoch 66/100\n",
      "3072/3072 [==============================] - 0s 118us/step - loss: 0.2359 - acc: 0.5771\n",
      "Epoch 67/100\n",
      "3072/3072 [==============================] - 1s 175us/step - loss: 0.2352 - acc: 0.5814\n",
      "Epoch 68/100\n",
      "3072/3072 [==============================] - 1s 181us/step - loss: 0.2349 - acc: 0.5794\n",
      "Epoch 69/100\n",
      "3072/3072 [==============================] - 0s 143us/step - loss: 0.2343 - acc: 0.5830\n",
      "Epoch 70/100\n",
      "3072/3072 [==============================] - 0s 128us/step - loss: 0.2346 - acc: 0.5736\n",
      "Epoch 71/100\n",
      "3072/3072 [==============================] - 0s 115us/step - loss: 0.2343 - acc: 0.5850\n",
      "Epoch 72/100\n",
      "3072/3072 [==============================] - 1s 179us/step - loss: 0.2335 - acc: 0.5768\n",
      "Epoch 73/100\n",
      "3072/3072 [==============================] - 0s 151us/step - loss: 0.2334 - acc: 0.5859\n",
      "Epoch 74/100\n",
      "3072/3072 [==============================] - 0s 152us/step - loss: 0.2325 - acc: 0.5798\n",
      "Epoch 75/100\n",
      "3072/3072 [==============================] - 0s 126us/step - loss: 0.2334 - acc: 0.5814\n",
      "Epoch 76/100\n",
      "3072/3072 [==============================] - 0s 120us/step - loss: 0.2328 - acc: 0.5827\n",
      "Epoch 77/100\n",
      "3072/3072 [==============================] - 0s 109us/step - loss: 0.2327 - acc: 0.5817\n",
      "Epoch 78/100\n",
      "3072/3072 [==============================] - 0s 113us/step - loss: 0.2318 - acc: 0.5856\n",
      "Epoch 79/100\n",
      "3072/3072 [==============================] - 0s 115us/step - loss: 0.2316 - acc: 0.5824\n",
      "Epoch 80/100\n",
      "3072/3072 [==============================] - 0s 115us/step - loss: 0.2310 - acc: 0.5938\n",
      "Epoch 81/100\n",
      "3072/3072 [==============================] - 0s 105us/step - loss: 0.2315 - acc: 0.5869\n",
      "Epoch 82/100\n",
      "3072/3072 [==============================] - 0s 113us/step - loss: 0.2301 - acc: 0.5876\n",
      "Epoch 83/100\n",
      "3072/3072 [==============================] - 0s 114us/step - loss: 0.2308 - acc: 0.5908\n",
      "Epoch 84/100\n",
      "3072/3072 [==============================] - 0s 104us/step - loss: 0.2305 - acc: 0.5941\n",
      "Epoch 85/100\n",
      "3072/3072 [==============================] - 0s 107us/step - loss: 0.2298 - acc: 0.5902\n",
      "Epoch 86/100\n",
      "3072/3072 [==============================] - 0s 118us/step - loss: 0.2297 - acc: 0.6006\n",
      "Epoch 87/100\n",
      "3072/3072 [==============================] - 0s 117us/step - loss: 0.2299 - acc: 0.5866\n",
      "Epoch 88/100\n",
      "3072/3072 [==============================] - 0s 124us/step - loss: 0.2295 - acc: 0.5872\n",
      "Epoch 89/100\n",
      "3072/3072 [==============================] - 0s 113us/step - loss: 0.2289 - acc: 0.5938\n",
      "Epoch 90/100\n",
      "3072/3072 [==============================] - 0s 109us/step - loss: 0.2289 - acc: 0.5977\n",
      "Epoch 91/100\n",
      "3072/3072 [==============================] - 0s 117us/step - loss: 0.2281 - acc: 0.5960\n",
      "Epoch 92/100\n",
      "3072/3072 [==============================] - 0s 125us/step - loss: 0.2277 - acc: 0.5990\n",
      "Epoch 93/100\n",
      "3072/3072 [==============================] - 0s 111us/step - loss: 0.2282 - acc: 0.5866\n",
      "Epoch 94/100\n",
      "3072/3072 [==============================] - 0s 112us/step - loss: 0.2282 - acc: 0.5967\n",
      "Epoch 95/100\n",
      "3072/3072 [==============================] - 0s 114us/step - loss: 0.2267 - acc: 0.6019\n",
      "Epoch 96/100\n",
      "3072/3072 [==============================] - 0s 115us/step - loss: 0.2274 - acc: 0.5941\n",
      "Epoch 97/100\n",
      "3072/3072 [==============================] - 0s 108us/step - loss: 0.2263 - acc: 0.5973\n",
      "Epoch 98/100\n",
      "3072/3072 [==============================] - 0s 113us/step - loss: 0.2264 - acc: 0.6003\n",
      "Epoch 99/100\n",
      "3072/3072 [==============================] - 0s 109us/step - loss: 0.2267 - acc: 0.5986\n",
      "Epoch 100/100\n",
      "3072/3072 [==============================] - 0s 118us/step - loss: 0.2263 - acc: 0.6055\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fb918454e50>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit and Train Model\n",
    "\n",
    "model.fit(X_train, y_train, \n",
    "          batch_size = batch_size, \n",
    "          epochs = epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter tuning\n",
    "\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, activation='relu', input_shape=(inputs,)))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='uniform', activation='sigmoid'))\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='mean_squared_error',\n",
    "                  metrics=['acc'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# prep for grid search\n",
    "\n",
    "clf = KerasClassifier(build_fn=create_model, verbose=0)\n",
    "\n",
    "# define Grid Search params\n",
    "\n",
    "param_grid = {'batch_size': [20, 60, 80, 100, 200],\n",
    "             'epochs': [20]}\n",
    "\n",
    "grid = GridSearchCV(estimator=clf, \n",
    "                    param_grid=param_grid,\n",
    "                    n_jobs=1)\n",
    "grid_result = grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.5149739583333334 using {'batch_size': 200, 'epochs': 20}\n",
      "Means: 0.5074869791666666, Stdev: 0.009172527866930581 with: {'batch_size': 20, 'epochs': 20}\n",
      "Means: 0.5068359375, Stdev: 0.006526672413985489 with: {'batch_size': 60, 'epochs': 20}\n",
      "Means: 0.50390625, Stdev: 0.010457277606906908 with: {'batch_size': 80, 'epochs': 20}\n",
      "Means: 0.5139973958333334, Stdev: 0.007146646614615934 with: {'batch_size': 100, 'epochs': 20}\n",
      "Means: 0.5149739583333334, Stdev: 0.003222491841344943 with: {'batch_size': 200, 'epochs': 20}\n"
     ]
    }
   ],
   "source": [
    "# report results\n",
    "\n",
    "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
    "\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "\n",
    "params = grid_result.cv_results_['params']\n",
    "\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tune optimizer\n",
    "\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "def create_model(learn_rate=0.001):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, activation='relu', input_shape=(inputs,)))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='uniform', activation='sigmoid'))\n",
    "    model.compile(optimizer=optimizers.Adam(lr=learn_rate),\n",
    "                  loss='mean_squared_error',\n",
    "                  metrics=['acc'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# prepare for grid search\n",
    "\n",
    "clf = KerasClassifier(build_fn=create_model, verbose=0)\n",
    "\n",
    "# optimize with optimizer\n",
    "\n",
    "param_grid = {'batch_size': [20], \n",
    "              'epochs': [20], \n",
    "              'learn_rate': [0.001, 0.01, 0.1, 0.2, 0.3, 0.5]}\n",
    "\n",
    "grid = GridSearchCV(estimator=clf, \n",
    "                    param_grid=param_grid, \n",
    "                    n_jobs=1)\n",
    "\n",
    "grid_result = grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.5179036458333334 using {'batch_size': 20, 'epochs': 20, 'learn_rate': 0.5}\n",
      "Means: 0.5166015625, Stdev: 0.010961886875314282 with: {'batch_size': 20, 'epochs': 20, 'learn_rate': 0.001}\n",
      "Means: 0.5133463541666666, Stdev: 0.0053090841244466435 with: {'batch_size': 20, 'epochs': 20, 'learn_rate': 0.01}\n",
      "Means: 0.4964192708333333, Stdev: 0.017835452515368132 with: {'batch_size': 20, 'epochs': 20, 'learn_rate': 0.1}\n",
      "Means: 0.4964192708333333, Stdev: 0.017835452515368132 with: {'batch_size': 20, 'epochs': 20, 'learn_rate': 0.2}\n",
      "Means: 0.4820963541666667, Stdev: 0.0032224918413449434 with: {'batch_size': 20, 'epochs': 20, 'learn_rate': 0.3}\n",
      "Means: 0.5179036458333334, Stdev: 0.003222491841344943 with: {'batch_size': 20, 'epochs': 20, 'learn_rate': 0.5}\n"
     ]
    }
   ],
   "source": [
    "# report results\n",
    "\n",
    "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tune epochs and save final model for future use\n",
    "\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, activation='relu', input_shape=(inputs,)))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='uniform', activation='sigmoid'))\n",
    "    model.compile(optimizer=optimizers.Adam(lr=0.01),\n",
    "                  loss='mean_squared_error',\n",
    "                  metrics=['acc'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare for grid search\n",
    "\n",
    "clf = KerasClassifier(build_fn=create_model, verbose=0)\n",
    "\n",
    "# define grid search params\n",
    "\n",
    "param_grid = {'batch_size': [20], \n",
    "              'epochs': [50, 100, 200, 500]}\n",
    "\n",
    "grid = GridSearchCV(estimator=clf, \n",
    "                    param_grid=param_grid,\n",
    "                    n_jobs=1)\n",
    "\n",
    "grid_result = grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.521484375 using {'batch_size': 20, 'epochs': 200}\n",
      "Means: 0.5126953125, Stdev: 0.0021096161127629755 with: {'batch_size': 20, 'epochs': 50}\n",
      "Means: 0.5078125, Stdev: 0.013601941676937616 with: {'batch_size': 20, 'epochs': 100}\n",
      "Means: 0.521484375, Stdev: 0.0007973599423122325 with: {'batch_size': 20, 'epochs': 200}\n",
      "Means: 0.5152994791666666, Stdev: 0.0024359748611809512 with: {'batch_size': 20, 'epochs': 500}\n"
     ]
    }
   ],
   "source": [
    "# Report Results\n",
    "\n",
    "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "3072/3072 [==============================] - 0s 97us/step - loss: 0.2229 - acc: 0.6104\n",
      "Epoch 2/500\n",
      "3072/3072 [==============================] - 0s 78us/step - loss: 0.2216 - acc: 0.6198\n",
      "Epoch 3/500\n",
      "3072/3072 [==============================] - 0s 76us/step - loss: 0.2211 - acc: 0.6169\n",
      "Epoch 4/500\n",
      "3072/3072 [==============================] - 0s 67us/step - loss: 0.2211 - acc: 0.6165\n",
      "Epoch 5/500\n",
      "3072/3072 [==============================] - 0s 66us/step - loss: 0.2211 - acc: 0.6185\n",
      "Epoch 6/500\n",
      "3072/3072 [==============================] - 0s 62us/step - loss: 0.2210 - acc: 0.6165\n",
      "Epoch 7/500\n",
      "3072/3072 [==============================] - 0s 66us/step - loss: 0.2204 - acc: 0.6126\n",
      "Epoch 8/500\n",
      "3072/3072 [==============================] - 0s 65us/step - loss: 0.2202 - acc: 0.6123\n",
      "Epoch 9/500\n",
      "3072/3072 [==============================] - 0s 72us/step - loss: 0.2203 - acc: 0.6185\n",
      "Epoch 10/500\n",
      "3072/3072 [==============================] - 0s 69us/step - loss: 0.2198 - acc: 0.6090\n",
      "Epoch 11/500\n",
      "3072/3072 [==============================] - 0s 72us/step - loss: 0.2204 - acc: 0.6191\n",
      "Epoch 12/500\n",
      "3072/3072 [==============================] - 0s 69us/step - loss: 0.2202 - acc: 0.6130\n",
      "Epoch 13/500\n",
      "3072/3072 [==============================] - 0s 70us/step - loss: 0.2198 - acc: 0.6201\n",
      "Epoch 14/500\n",
      "3072/3072 [==============================] - 0s 73us/step - loss: 0.2192 - acc: 0.6149\n",
      "Epoch 15/500\n",
      "3072/3072 [==============================] - 0s 76us/step - loss: 0.2193 - acc: 0.6191\n",
      "Epoch 16/500\n",
      "3072/3072 [==============================] - 0s 71us/step - loss: 0.2189 - acc: 0.6175\n",
      "Epoch 17/500\n",
      "3072/3072 [==============================] - 0s 76us/step - loss: 0.2179 - acc: 0.6266\n",
      "Epoch 18/500\n",
      "3072/3072 [==============================] - 0s 72us/step - loss: 0.2186 - acc: 0.6172\n",
      "Epoch 19/500\n",
      "3072/3072 [==============================] - 0s 75us/step - loss: 0.2186 - acc: 0.6169\n",
      "Epoch 20/500\n",
      "3072/3072 [==============================] - 0s 76us/step - loss: 0.2173 - acc: 0.6159\n",
      "Epoch 21/500\n",
      "3072/3072 [==============================] - 0s 76us/step - loss: 0.2175 - acc: 0.6299\n",
      "Epoch 22/500\n",
      "3072/3072 [==============================] - 0s 76us/step - loss: 0.2178 - acc: 0.6165\n",
      "Epoch 23/500\n",
      "3072/3072 [==============================] - 0s 78us/step - loss: 0.2169 - acc: 0.6204\n",
      "Epoch 24/500\n",
      "3072/3072 [==============================] - 0s 70us/step - loss: 0.2167 - acc: 0.6279\n",
      "Epoch 25/500\n",
      "3072/3072 [==============================] - 0s 69us/step - loss: 0.2168 - acc: 0.6260\n",
      "Epoch 26/500\n",
      "3072/3072 [==============================] - 0s 73us/step - loss: 0.2170 - acc: 0.6250\n",
      "Epoch 27/500\n",
      "3072/3072 [==============================] - 0s 72us/step - loss: 0.2160 - acc: 0.6283\n",
      "Epoch 28/500\n",
      "3072/3072 [==============================] - 0s 75us/step - loss: 0.2154 - acc: 0.6191\n",
      "Epoch 29/500\n",
      "3072/3072 [==============================] - 0s 79us/step - loss: 0.2152 - acc: 0.6318\n",
      "Epoch 30/500\n",
      "3072/3072 [==============================] - 0s 78us/step - loss: 0.2155 - acc: 0.6234\n",
      "Epoch 31/500\n",
      "3072/3072 [==============================] - 0s 80us/step - loss: 0.2150 - acc: 0.6230\n",
      "Epoch 32/500\n",
      "3072/3072 [==============================] - 0s 77us/step - loss: 0.2144 - acc: 0.6312\n",
      "Epoch 33/500\n",
      "3072/3072 [==============================] - 0s 72us/step - loss: 0.2156 - acc: 0.6266\n",
      "Epoch 34/500\n",
      "3072/3072 [==============================] - 0s 75us/step - loss: 0.2145 - acc: 0.6335\n",
      "Epoch 35/500\n",
      "3072/3072 [==============================] - 0s 74us/step - loss: 0.2139 - acc: 0.6266\n",
      "Epoch 36/500\n",
      "3072/3072 [==============================] - 0s 73us/step - loss: 0.2141 - acc: 0.6309\n",
      "Epoch 37/500\n",
      "3072/3072 [==============================] - 0s 72us/step - loss: 0.2136 - acc: 0.6338\n",
      "Epoch 38/500\n",
      "3072/3072 [==============================] - 0s 73us/step - loss: 0.2140 - acc: 0.6240\n",
      "Epoch 39/500\n",
      "3072/3072 [==============================] - 0s 71us/step - loss: 0.2129 - acc: 0.6318\n",
      "Epoch 40/500\n",
      "3072/3072 [==============================] - 0s 83us/step - loss: 0.2138 - acc: 0.6331\n",
      "Epoch 41/500\n",
      "3072/3072 [==============================] - 0s 77us/step - loss: 0.2131 - acc: 0.6309\n",
      "Epoch 42/500\n",
      "3072/3072 [==============================] - 0s 80us/step - loss: 0.2130 - acc: 0.6325\n",
      "Epoch 43/500\n",
      "3072/3072 [==============================] - 0s 74us/step - loss: 0.2121 - acc: 0.6312\n",
      "Epoch 44/500\n",
      "3072/3072 [==============================] - 0s 80us/step - loss: 0.2117 - acc: 0.6364\n",
      "Epoch 45/500\n",
      "3072/3072 [==============================] - 0s 72us/step - loss: 0.2121 - acc: 0.6322\n",
      "Epoch 46/500\n",
      "3072/3072 [==============================] - 0s 78us/step - loss: 0.2108 - acc: 0.6374\n",
      "Epoch 47/500\n",
      "3072/3072 [==============================] - 0s 72us/step - loss: 0.2123 - acc: 0.6286\n",
      "Epoch 48/500\n",
      "3072/3072 [==============================] - 0s 74us/step - loss: 0.2107 - acc: 0.6396\n",
      "Epoch 49/500\n",
      "3072/3072 [==============================] - 0s 72us/step - loss: 0.2103 - acc: 0.6390\n",
      "Epoch 50/500\n",
      "3072/3072 [==============================] - 0s 76us/step - loss: 0.2105 - acc: 0.6439\n",
      "Epoch 51/500\n",
      "3072/3072 [==============================] - 0s 76us/step - loss: 0.2101 - acc: 0.6318\n",
      "Epoch 52/500\n",
      "3072/3072 [==============================] - 0s 73us/step - loss: 0.2106 - acc: 0.6390\n",
      "Epoch 53/500\n",
      "3072/3072 [==============================] - 0s 71us/step - loss: 0.2107 - acc: 0.6309\n",
      "Epoch 54/500\n",
      "3072/3072 [==============================] - 0s 76us/step - loss: 0.2091 - acc: 0.6445\n",
      "Epoch 55/500\n",
      "3072/3072 [==============================] - 0s 71us/step - loss: 0.2098 - acc: 0.6361\n",
      "Epoch 56/500\n",
      "3072/3072 [==============================] - 0s 76us/step - loss: 0.2090 - acc: 0.6361\n",
      "Epoch 57/500\n",
      "3072/3072 [==============================] - 0s 75us/step - loss: 0.2090 - acc: 0.6419\n",
      "Epoch 58/500\n",
      "3072/3072 [==============================] - 0s 79us/step - loss: 0.2085 - acc: 0.6416\n",
      "Epoch 59/500\n",
      "3072/3072 [==============================] - 0s 81us/step - loss: 0.2080 - acc: 0.6429\n",
      "Epoch 60/500\n",
      "3072/3072 [==============================] - 0s 75us/step - loss: 0.2076 - acc: 0.6465\n",
      "Epoch 61/500\n",
      "3072/3072 [==============================] - 0s 75us/step - loss: 0.2088 - acc: 0.6380\n",
      "Epoch 62/500\n",
      "3072/3072 [==============================] - 0s 72us/step - loss: 0.2082 - acc: 0.6439\n",
      "Epoch 63/500\n",
      "3072/3072 [==============================] - 0s 71us/step - loss: 0.2078 - acc: 0.6494\n",
      "Epoch 64/500\n",
      "3072/3072 [==============================] - 0s 73us/step - loss: 0.2078 - acc: 0.6478\n",
      "Epoch 65/500\n",
      "3072/3072 [==============================] - 0s 70us/step - loss: 0.2060 - acc: 0.6536\n",
      "Epoch 66/500\n",
      "3072/3072 [==============================] - 0s 75us/step - loss: 0.2065 - acc: 0.6458\n",
      "Epoch 67/500\n",
      "3072/3072 [==============================] - 0s 77us/step - loss: 0.2065 - acc: 0.6510\n",
      "Epoch 68/500\n",
      "3072/3072 [==============================] - 0s 69us/step - loss: 0.2063 - acc: 0.6523\n",
      "Epoch 69/500\n",
      "3072/3072 [==============================] - 0s 72us/step - loss: 0.2062 - acc: 0.6432\n",
      "Epoch 70/500\n",
      "3072/3072 [==============================] - 0s 75us/step - loss: 0.2055 - acc: 0.6468\n",
      "Epoch 71/500\n",
      "3072/3072 [==============================] - 0s 73us/step - loss: 0.2059 - acc: 0.6468\n",
      "Epoch 72/500\n",
      "3072/3072 [==============================] - 0s 77us/step - loss: 0.2062 - acc: 0.6478\n",
      "Epoch 73/500\n",
      "3072/3072 [==============================] - 0s 74us/step - loss: 0.2053 - acc: 0.6559\n",
      "Epoch 74/500\n",
      "3072/3072 [==============================] - 0s 68us/step - loss: 0.2055 - acc: 0.6497\n",
      "Epoch 75/500\n",
      "3072/3072 [==============================] - 0s 73us/step - loss: 0.2053 - acc: 0.6475\n",
      "Epoch 76/500\n",
      "3072/3072 [==============================] - 0s 72us/step - loss: 0.2040 - acc: 0.6553\n",
      "Epoch 77/500\n",
      "3072/3072 [==============================] - 0s 73us/step - loss: 0.2043 - acc: 0.6540\n",
      "Epoch 78/500\n",
      "3072/3072 [==============================] - 0s 70us/step - loss: 0.2044 - acc: 0.6556\n",
      "Epoch 79/500\n",
      "3072/3072 [==============================] - 0s 73us/step - loss: 0.2031 - acc: 0.6507\n",
      "Epoch 80/500\n",
      "3072/3072 [==============================] - 0s 73us/step - loss: 0.2038 - acc: 0.6549\n",
      "Epoch 81/500\n",
      "3072/3072 [==============================] - 0s 70us/step - loss: 0.2024 - acc: 0.6572\n",
      "Epoch 82/500\n",
      "3072/3072 [==============================] - 0s 72us/step - loss: 0.2021 - acc: 0.6576\n",
      "Epoch 83/500\n",
      "3072/3072 [==============================] - 0s 73us/step - loss: 0.2038 - acc: 0.6540\n",
      "Epoch 84/500\n",
      "3072/3072 [==============================] - 0s 69us/step - loss: 0.2037 - acc: 0.6520\n",
      "Epoch 85/500\n",
      "3072/3072 [==============================] - 0s 65us/step - loss: 0.2028 - acc: 0.6556\n",
      "Epoch 86/500\n",
      "3072/3072 [==============================] - 0s 78us/step - loss: 0.2018 - acc: 0.6637\n",
      "Epoch 87/500\n",
      "3072/3072 [==============================] - 0s 69us/step - loss: 0.2014 - acc: 0.6631\n",
      "Epoch 88/500\n",
      "3072/3072 [==============================] - 0s 79us/step - loss: 0.2023 - acc: 0.6510\n",
      "Epoch 89/500\n",
      "3072/3072 [==============================] - 0s 70us/step - loss: 0.2029 - acc: 0.6527\n",
      "Epoch 90/500\n",
      "3072/3072 [==============================] - 0s 74us/step - loss: 0.2020 - acc: 0.6579\n",
      "Epoch 91/500\n",
      "3072/3072 [==============================] - 0s 70us/step - loss: 0.2010 - acc: 0.6644\n",
      "Epoch 92/500\n",
      "3072/3072 [==============================] - 0s 75us/step - loss: 0.1999 - acc: 0.6611\n",
      "Epoch 93/500\n",
      "3072/3072 [==============================] - 0s 70us/step - loss: 0.2012 - acc: 0.6634\n",
      "Epoch 94/500\n",
      "3072/3072 [==============================] - 0s 71us/step - loss: 0.2005 - acc: 0.6530\n",
      "Epoch 95/500\n",
      "3072/3072 [==============================] - 0s 72us/step - loss: 0.1994 - acc: 0.6654\n",
      "Epoch 96/500\n",
      "3072/3072 [==============================] - 0s 73us/step - loss: 0.1998 - acc: 0.6647\n",
      "Epoch 97/500\n",
      "3072/3072 [==============================] - 0s 70us/step - loss: 0.1980 - acc: 0.6709\n",
      "Epoch 98/500\n",
      "3072/3072 [==============================] - 0s 72us/step - loss: 0.1982 - acc: 0.6712\n",
      "Epoch 99/500\n",
      "3072/3072 [==============================] - 0s 75us/step - loss: 0.1996 - acc: 0.6670\n",
      "Epoch 100/500\n",
      "3072/3072 [==============================] - 0s 70us/step - loss: 0.2006 - acc: 0.6585\n",
      "Epoch 101/500\n",
      "3072/3072 [==============================] - 0s 72us/step - loss: 0.1983 - acc: 0.6709\n",
      "Epoch 102/500\n",
      "3072/3072 [==============================] - 0s 68us/step - loss: 0.1989 - acc: 0.6618\n",
      "Epoch 103/500\n",
      "3072/3072 [==============================] - 0s 71us/step - loss: 0.1977 - acc: 0.6650\n",
      "Epoch 104/500\n",
      "3072/3072 [==============================] - 0s 72us/step - loss: 0.1986 - acc: 0.6654\n",
      "Epoch 105/500\n",
      "3072/3072 [==============================] - 0s 78us/step - loss: 0.1972 - acc: 0.6650\n",
      "Epoch 106/500\n",
      "3072/3072 [==============================] - 0s 74us/step - loss: 0.1978 - acc: 0.6605\n",
      "Epoch 107/500\n",
      "3072/3072 [==============================] - 0s 72us/step - loss: 0.1975 - acc: 0.6611\n",
      "Epoch 108/500\n",
      "3072/3072 [==============================] - 0s 69us/step - loss: 0.1974 - acc: 0.6670\n",
      "Epoch 109/500\n",
      "3072/3072 [==============================] - 0s 72us/step - loss: 0.1961 - acc: 0.6686\n",
      "Epoch 110/500\n",
      "3072/3072 [==============================] - 0s 70us/step - loss: 0.1978 - acc: 0.6670\n",
      "Epoch 111/500\n",
      "3072/3072 [==============================] - 0s 77us/step - loss: 0.1971 - acc: 0.6706\n",
      "Epoch 112/500\n",
      "3072/3072 [==============================] - 0s 78us/step - loss: 0.1964 - acc: 0.6729\n",
      "Epoch 113/500\n",
      "3072/3072 [==============================] - 0s 70us/step - loss: 0.1975 - acc: 0.6637\n",
      "Epoch 114/500\n",
      "3072/3072 [==============================] - 0s 71us/step - loss: 0.1960 - acc: 0.6680\n",
      "Epoch 115/500\n",
      "3072/3072 [==============================] - 0s 75us/step - loss: 0.1954 - acc: 0.6696\n",
      "Epoch 116/500\n",
      "3072/3072 [==============================] - 0s 80us/step - loss: 0.1948 - acc: 0.6686\n",
      "Epoch 117/500\n",
      "3072/3072 [==============================] - 0s 78us/step - loss: 0.1946 - acc: 0.6657\n",
      "Epoch 118/500\n",
      "3072/3072 [==============================] - 0s 70us/step - loss: 0.1944 - acc: 0.6735\n",
      "Epoch 119/500\n",
      "3072/3072 [==============================] - 0s 74us/step - loss: 0.1954 - acc: 0.6673\n",
      "Epoch 120/500\n",
      "3072/3072 [==============================] - 0s 74us/step - loss: 0.1960 - acc: 0.6654\n",
      "Epoch 121/500\n",
      "3072/3072 [==============================] - 0s 74us/step - loss: 0.1942 - acc: 0.6742\n",
      "Epoch 122/500\n",
      "3072/3072 [==============================] - 0s 70us/step - loss: 0.1951 - acc: 0.6774\n",
      "Epoch 123/500\n",
      "3072/3072 [==============================] - 0s 70us/step - loss: 0.1951 - acc: 0.6761\n",
      "Epoch 124/500\n",
      "3072/3072 [==============================] - 0s 72us/step - loss: 0.1950 - acc: 0.6657\n",
      "Epoch 125/500\n",
      "3072/3072 [==============================] - 0s 68us/step - loss: 0.1934 - acc: 0.6738\n",
      "Epoch 126/500\n",
      "3072/3072 [==============================] - 0s 73us/step - loss: 0.1945 - acc: 0.6709\n",
      "Epoch 127/500\n",
      "3072/3072 [==============================] - 0s 74us/step - loss: 0.1929 - acc: 0.6784\n",
      "Epoch 128/500\n",
      "3072/3072 [==============================] - 0s 72us/step - loss: 0.1937 - acc: 0.6787\n",
      "Epoch 129/500\n",
      "3072/3072 [==============================] - 0s 68us/step - loss: 0.1929 - acc: 0.6829\n",
      "Epoch 130/500\n",
      "3072/3072 [==============================] - 0s 75us/step - loss: 0.1927 - acc: 0.6790\n",
      "Epoch 131/500\n",
      "3072/3072 [==============================] - 0s 78us/step - loss: 0.1920 - acc: 0.6823\n",
      "Epoch 132/500\n",
      "3072/3072 [==============================] - 0s 67us/step - loss: 0.1928 - acc: 0.6787\n",
      "Epoch 133/500\n",
      "3072/3072 [==============================] - 0s 73us/step - loss: 0.1917 - acc: 0.6797\n",
      "Epoch 134/500\n",
      "3072/3072 [==============================] - 0s 69us/step - loss: 0.1913 - acc: 0.6800\n",
      "Epoch 135/500\n",
      "3072/3072 [==============================] - 0s 72us/step - loss: 0.1918 - acc: 0.6829\n",
      "Epoch 136/500\n",
      "3072/3072 [==============================] - 0s 73us/step - loss: 0.1929 - acc: 0.6738\n",
      "Epoch 137/500\n",
      "3072/3072 [==============================] - 0s 71us/step - loss: 0.1914 - acc: 0.6807\n",
      "Epoch 138/500\n",
      "3072/3072 [==============================] - 0s 73us/step - loss: 0.1922 - acc: 0.6852\n",
      "Epoch 139/500\n",
      "3072/3072 [==============================] - 0s 72us/step - loss: 0.1904 - acc: 0.6813\n",
      "Epoch 140/500\n",
      "3072/3072 [==============================] - 0s 74us/step - loss: 0.1894 - acc: 0.6852\n",
      "Epoch 141/500\n",
      "3072/3072 [==============================] - 0s 73us/step - loss: 0.1899 - acc: 0.6771\n",
      "Epoch 142/500\n",
      "3072/3072 [==============================] - 0s 75us/step - loss: 0.1905 - acc: 0.6833\n",
      "Epoch 143/500\n",
      "3072/3072 [==============================] - 0s 90us/step - loss: 0.1902 - acc: 0.6794\n",
      "Epoch 144/500\n",
      "3072/3072 [==============================] - 0s 117us/step - loss: 0.1902 - acc: 0.6803\n",
      "Epoch 145/500\n",
      "3072/3072 [==============================] - 0s 80us/step - loss: 0.1903 - acc: 0.6865\n",
      "Epoch 146/500\n",
      "3072/3072 [==============================] - 0s 115us/step - loss: 0.1898 - acc: 0.6849\n",
      "Epoch 147/500\n",
      "3072/3072 [==============================] - 0s 96us/step - loss: 0.1903 - acc: 0.6803\n",
      "Epoch 148/500\n",
      "3072/3072 [==============================] - 0s 105us/step - loss: 0.1890 - acc: 0.6885\n",
      "Epoch 149/500\n",
      "3072/3072 [==============================] - 0s 112us/step - loss: 0.1894 - acc: 0.6839\n",
      "Epoch 150/500\n",
      "3072/3072 [==============================] - 0s 82us/step - loss: 0.1885 - acc: 0.6882\n",
      "Epoch 151/500\n",
      "3072/3072 [==============================] - 0s 80us/step - loss: 0.1898 - acc: 0.6855\n",
      "Epoch 152/500\n",
      "3072/3072 [==============================] - 0s 78us/step - loss: 0.1891 - acc: 0.6875\n",
      "Epoch 153/500\n",
      "3072/3072 [==============================] - 0s 80us/step - loss: 0.1881 - acc: 0.6872\n",
      "Epoch 154/500\n",
      "3072/3072 [==============================] - 0s 76us/step - loss: 0.1881 - acc: 0.6816\n",
      "Epoch 155/500\n",
      "3072/3072 [==============================] - 0s 75us/step - loss: 0.1891 - acc: 0.6875\n",
      "Epoch 156/500\n",
      "3072/3072 [==============================] - 0s 87us/step - loss: 0.1868 - acc: 0.6882\n",
      "Epoch 157/500\n",
      "3072/3072 [==============================] - 0s 111us/step - loss: 0.1862 - acc: 0.6930\n",
      "Epoch 158/500\n",
      "3072/3072 [==============================] - 0s 77us/step - loss: 0.1876 - acc: 0.6842\n",
      "Epoch 159/500\n",
      "3072/3072 [==============================] - 0s 74us/step - loss: 0.1875 - acc: 0.6930\n",
      "Epoch 160/500\n",
      "3072/3072 [==============================] - 0s 98us/step - loss: 0.1867 - acc: 0.6833\n",
      "Epoch 161/500\n",
      "3072/3072 [==============================] - 0s 94us/step - loss: 0.1873 - acc: 0.6904\n",
      "Epoch 162/500\n",
      "3072/3072 [==============================] - 0s 73us/step - loss: 0.1865 - acc: 0.6885\n",
      "Epoch 163/500\n",
      "3072/3072 [==============================] - 0s 79us/step - loss: 0.1859 - acc: 0.6862\n",
      "Epoch 164/500\n",
      "3072/3072 [==============================] - 0s 79us/step - loss: 0.1851 - acc: 0.6917\n",
      "Epoch 165/500\n",
      "3072/3072 [==============================] - 0s 72us/step - loss: 0.1850 - acc: 0.7005\n",
      "Epoch 166/500\n",
      "3072/3072 [==============================] - 0s 78us/step - loss: 0.1868 - acc: 0.6924\n",
      "Epoch 167/500\n",
      "3072/3072 [==============================] - 0s 77us/step - loss: 0.1872 - acc: 0.6852\n",
      "Epoch 168/500\n",
      "3072/3072 [==============================] - 0s 74us/step - loss: 0.1867 - acc: 0.6943\n",
      "Epoch 169/500\n",
      "3072/3072 [==============================] - 0s 74us/step - loss: 0.1853 - acc: 0.6901\n",
      "Epoch 170/500\n",
      "3072/3072 [==============================] - 0s 83us/step - loss: 0.1846 - acc: 0.6917\n",
      "Epoch 171/500\n",
      "3072/3072 [==============================] - 0s 81us/step - loss: 0.1849 - acc: 0.6908\n",
      "Epoch 172/500\n",
      "3072/3072 [==============================] - 0s 80us/step - loss: 0.1845 - acc: 0.6966\n",
      "Epoch 173/500\n",
      "3072/3072 [==============================] - 0s 95us/step - loss: 0.1839 - acc: 0.6966\n",
      "Epoch 174/500\n",
      "3072/3072 [==============================] - 0s 107us/step - loss: 0.1841 - acc: 0.6924\n",
      "Epoch 175/500\n",
      "3072/3072 [==============================] - 0s 78us/step - loss: 0.1844 - acc: 0.6930\n",
      "Epoch 176/500\n",
      "3072/3072 [==============================] - 0s 71us/step - loss: 0.1852 - acc: 0.6934\n",
      "Epoch 177/500\n",
      "3072/3072 [==============================] - 0s 68us/step - loss: 0.1855 - acc: 0.6950\n",
      "Epoch 178/500\n",
      "3072/3072 [==============================] - 0s 74us/step - loss: 0.1852 - acc: 0.6943\n",
      "Epoch 179/500\n",
      "3072/3072 [==============================] - 0s 74us/step - loss: 0.1836 - acc: 0.6924\n",
      "Epoch 180/500\n",
      "3072/3072 [==============================] - 0s 74us/step - loss: 0.1822 - acc: 0.6982\n",
      "Epoch 181/500\n",
      "3072/3072 [==============================] - 0s 72us/step - loss: 0.1837 - acc: 0.6921\n",
      "Epoch 182/500\n",
      "3072/3072 [==============================] - 0s 67us/step - loss: 0.1829 - acc: 0.7048\n",
      "Epoch 183/500\n",
      "3072/3072 [==============================] - 0s 72us/step - loss: 0.1822 - acc: 0.6976\n",
      "Epoch 184/500\n",
      "3072/3072 [==============================] - 0s 77us/step - loss: 0.1840 - acc: 0.6895\n",
      "Epoch 185/500\n",
      "3072/3072 [==============================] - 0s 74us/step - loss: 0.1830 - acc: 0.6960\n",
      "Epoch 186/500\n",
      "3072/3072 [==============================] - 0s 70us/step - loss: 0.1828 - acc: 0.6956\n",
      "Epoch 187/500\n",
      "3072/3072 [==============================] - 0s 75us/step - loss: 0.1816 - acc: 0.7002\n",
      "Epoch 188/500\n",
      "3072/3072 [==============================] - 0s 97us/step - loss: 0.1840 - acc: 0.6934\n",
      "Epoch 189/500\n",
      "3072/3072 [==============================] - 0s 84us/step - loss: 0.1830 - acc: 0.6943\n",
      "Epoch 190/500\n",
      "3072/3072 [==============================] - 0s 75us/step - loss: 0.1812 - acc: 0.7005\n",
      "Epoch 191/500\n",
      "3072/3072 [==============================] - 0s 78us/step - loss: 0.1809 - acc: 0.6973\n",
      "Epoch 192/500\n",
      "3072/3072 [==============================] - 0s 73us/step - loss: 0.1810 - acc: 0.7057\n",
      "Epoch 193/500\n",
      "3072/3072 [==============================] - 0s 81us/step - loss: 0.1811 - acc: 0.6999\n",
      "Epoch 194/500\n",
      "3072/3072 [==============================] - 0s 74us/step - loss: 0.1816 - acc: 0.6986\n",
      "Epoch 195/500\n",
      "3072/3072 [==============================] - 0s 77us/step - loss: 0.1802 - acc: 0.7048\n",
      "Epoch 196/500\n",
      "3072/3072 [==============================] - 0s 75us/step - loss: 0.1818 - acc: 0.7021\n",
      "Epoch 197/500\n",
      "3072/3072 [==============================] - 0s 75us/step - loss: 0.1803 - acc: 0.7031\n",
      "Epoch 198/500\n",
      "3072/3072 [==============================] - 0s 74us/step - loss: 0.1808 - acc: 0.6956\n",
      "Epoch 199/500\n",
      "3072/3072 [==============================] - 0s 83us/step - loss: 0.1811 - acc: 0.6963\n",
      "Epoch 200/500\n",
      "3072/3072 [==============================] - 0s 75us/step - loss: 0.1794 - acc: 0.7018\n",
      "Epoch 201/500\n",
      "3072/3072 [==============================] - 0s 80us/step - loss: 0.1790 - acc: 0.7035\n",
      "Epoch 202/500\n",
      "3072/3072 [==============================] - 0s 82us/step - loss: 0.1804 - acc: 0.7015\n",
      "Epoch 203/500\n",
      "3072/3072 [==============================] - 0s 78us/step - loss: 0.1808 - acc: 0.6992\n",
      "Epoch 204/500\n",
      "3072/3072 [==============================] - 0s 94us/step - loss: 0.1792 - acc: 0.7041\n",
      "Epoch 205/500\n",
      "3072/3072 [==============================] - 0s 130us/step - loss: 0.1813 - acc: 0.6989\n",
      "Epoch 206/500\n",
      "3072/3072 [==============================] - 0s 78us/step - loss: 0.1795 - acc: 0.7008\n",
      "Epoch 207/500\n",
      "3072/3072 [==============================] - 0s 78us/step - loss: 0.1795 - acc: 0.7031\n",
      "Epoch 208/500\n",
      "3072/3072 [==============================] - 0s 75us/step - loss: 0.1807 - acc: 0.6960\n",
      "Epoch 209/500\n",
      "3072/3072 [==============================] - 0s 74us/step - loss: 0.1784 - acc: 0.7044\n",
      "Epoch 210/500\n",
      "3072/3072 [==============================] - 0s 80us/step - loss: 0.1797 - acc: 0.6917\n",
      "Epoch 211/500\n",
      "3072/3072 [==============================] - 0s 85us/step - loss: 0.1780 - acc: 0.7028\n",
      "Epoch 212/500\n",
      "3072/3072 [==============================] - 0s 85us/step - loss: 0.1783 - acc: 0.7035\n",
      "Epoch 213/500\n",
      "3072/3072 [==============================] - 0s 78us/step - loss: 0.1785 - acc: 0.7008\n",
      "Epoch 214/500\n",
      "3072/3072 [==============================] - 0s 77us/step - loss: 0.1784 - acc: 0.7054\n",
      "Epoch 215/500\n",
      "3072/3072 [==============================] - 0s 72us/step - loss: 0.1791 - acc: 0.7070\n",
      "Epoch 216/500\n",
      "3072/3072 [==============================] - 0s 76us/step - loss: 0.1772 - acc: 0.7093\n",
      "Epoch 217/500\n",
      "3072/3072 [==============================] - 0s 75us/step - loss: 0.1784 - acc: 0.7002\n",
      "Epoch 218/500\n",
      "3072/3072 [==============================] - 0s 77us/step - loss: 0.1779 - acc: 0.6969\n",
      "Epoch 219/500\n",
      "3072/3072 [==============================] - 0s 74us/step - loss: 0.1771 - acc: 0.7096\n",
      "Epoch 220/500\n",
      "3072/3072 [==============================] - 0s 72us/step - loss: 0.1774 - acc: 0.7008\n",
      "Epoch 221/500\n",
      "3072/3072 [==============================] - 0s 78us/step - loss: 0.1767 - acc: 0.7100\n",
      "Epoch 222/500\n",
      "3072/3072 [==============================] - 0s 76us/step - loss: 0.1763 - acc: 0.7008\n",
      "Epoch 223/500\n",
      "3072/3072 [==============================] - 0s 79us/step - loss: 0.1760 - acc: 0.7083\n",
      "Epoch 224/500\n",
      "3072/3072 [==============================] - 0s 75us/step - loss: 0.1763 - acc: 0.7054\n",
      "Epoch 225/500\n",
      "3072/3072 [==============================] - 0s 73us/step - loss: 0.1760 - acc: 0.7067\n",
      "Epoch 226/500\n",
      "3072/3072 [==============================] - 0s 78us/step - loss: 0.1754 - acc: 0.7116\n",
      "Epoch 227/500\n",
      "3072/3072 [==============================] - 0s 79us/step - loss: 0.1760 - acc: 0.7077\n",
      "Epoch 228/500\n",
      "3072/3072 [==============================] - 0s 79us/step - loss: 0.1760 - acc: 0.7041\n",
      "Epoch 229/500\n",
      "3072/3072 [==============================] - 0s 79us/step - loss: 0.1759 - acc: 0.7077\n",
      "Epoch 230/500\n",
      "3072/3072 [==============================] - 0s 75us/step - loss: 0.1761 - acc: 0.7080\n",
      "Epoch 231/500\n",
      "3072/3072 [==============================] - 0s 81us/step - loss: 0.1761 - acc: 0.7074\n",
      "Epoch 232/500\n",
      "3072/3072 [==============================] - 0s 92us/step - loss: 0.1765 - acc: 0.7028\n",
      "Epoch 233/500\n",
      "3072/3072 [==============================] - 0s 78us/step - loss: 0.1748 - acc: 0.7103\n",
      "Epoch 234/500\n",
      "3072/3072 [==============================] - 0s 80us/step - loss: 0.1750 - acc: 0.7080\n",
      "Epoch 235/500\n",
      "3072/3072 [==============================] - 0s 80us/step - loss: 0.1757 - acc: 0.7080\n",
      "Epoch 236/500\n",
      "3072/3072 [==============================] - 0s 78us/step - loss: 0.1751 - acc: 0.7116\n",
      "Epoch 237/500\n",
      "3072/3072 [==============================] - 0s 74us/step - loss: 0.1760 - acc: 0.7054\n",
      "Epoch 238/500\n",
      "3072/3072 [==============================] - 0s 75us/step - loss: 0.1757 - acc: 0.7061\n",
      "Epoch 239/500\n",
      "3072/3072 [==============================] - 0s 73us/step - loss: 0.1756 - acc: 0.7041\n",
      "Epoch 240/500\n",
      "3072/3072 [==============================] - 0s 75us/step - loss: 0.1757 - acc: 0.7064\n",
      "Epoch 241/500\n",
      "3072/3072 [==============================] - 0s 82us/step - loss: 0.1739 - acc: 0.7148\n",
      "Epoch 242/500\n",
      "3072/3072 [==============================] - 0s 72us/step - loss: 0.1743 - acc: 0.7119\n",
      "Epoch 243/500\n",
      "3072/3072 [==============================] - 0s 75us/step - loss: 0.1783 - acc: 0.6989\n",
      "Epoch 244/500\n",
      "3072/3072 [==============================] - 0s 71us/step - loss: 0.1744 - acc: 0.7096\n",
      "Epoch 245/500\n",
      "3072/3072 [==============================] - 0s 79us/step - loss: 0.1748 - acc: 0.7171\n",
      "Epoch 246/500\n",
      "3072/3072 [==============================] - 0s 81us/step - loss: 0.1749 - acc: 0.7093\n",
      "Epoch 247/500\n",
      "3072/3072 [==============================] - 0s 77us/step - loss: 0.1742 - acc: 0.7093\n",
      "Epoch 248/500\n",
      "3072/3072 [==============================] - 0s 83us/step - loss: 0.1742 - acc: 0.7090\n",
      "Epoch 249/500\n",
      "3072/3072 [==============================] - 0s 70us/step - loss: 0.1761 - acc: 0.7057\n",
      "Epoch 250/500\n",
      "3072/3072 [==============================] - 0s 74us/step - loss: 0.1732 - acc: 0.7158\n",
      "Epoch 251/500\n",
      "3072/3072 [==============================] - 0s 72us/step - loss: 0.1734 - acc: 0.7067\n",
      "Epoch 252/500\n",
      "3072/3072 [==============================] - 0s 73us/step - loss: 0.1737 - acc: 0.7090\n",
      "Epoch 253/500\n",
      "3072/3072 [==============================] - 0s 85us/step - loss: 0.1720 - acc: 0.7158\n",
      "Epoch 254/500\n",
      "3072/3072 [==============================] - 0s 97us/step - loss: 0.1729 - acc: 0.7171\n",
      "Epoch 255/500\n",
      "3072/3072 [==============================] - 0s 72us/step - loss: 0.1725 - acc: 0.7152\n",
      "Epoch 256/500\n",
      "3072/3072 [==============================] - 0s 77us/step - loss: 0.1726 - acc: 0.7126\n",
      "Epoch 257/500\n",
      "3072/3072 [==============================] - 0s 74us/step - loss: 0.1728 - acc: 0.7116\n",
      "Epoch 258/500\n",
      "3072/3072 [==============================] - 0s 82us/step - loss: 0.1729 - acc: 0.7096\n",
      "Epoch 259/500\n",
      "3072/3072 [==============================] - 0s 75us/step - loss: 0.1745 - acc: 0.7090\n",
      "Epoch 260/500\n",
      "3072/3072 [==============================] - 0s 75us/step - loss: 0.1711 - acc: 0.7148\n",
      "Epoch 261/500\n",
      "3072/3072 [==============================] - 0s 74us/step - loss: 0.1701 - acc: 0.7178\n",
      "Epoch 262/500\n",
      "3072/3072 [==============================] - 0s 79us/step - loss: 0.1716 - acc: 0.7080\n",
      "Epoch 263/500\n",
      "3072/3072 [==============================] - 0s 83us/step - loss: 0.1706 - acc: 0.7217\n",
      "Epoch 264/500\n",
      "3072/3072 [==============================] - 0s 75us/step - loss: 0.1725 - acc: 0.7171\n",
      "Epoch 265/500\n",
      "3072/3072 [==============================] - 0s 72us/step - loss: 0.1725 - acc: 0.7074\n",
      "Epoch 266/500\n",
      "3072/3072 [==============================] - 0s 74us/step - loss: 0.1722 - acc: 0.7139\n",
      "Epoch 267/500\n",
      "3072/3072 [==============================] - 0s 78us/step - loss: 0.1707 - acc: 0.7227\n",
      "Epoch 268/500\n",
      "3072/3072 [==============================] - 0s 71us/step - loss: 0.1708 - acc: 0.7132\n",
      "Epoch 269/500\n",
      "3072/3072 [==============================] - 0s 77us/step - loss: 0.1697 - acc: 0.7191\n",
      "Epoch 270/500\n",
      "3072/3072 [==============================] - 0s 73us/step - loss: 0.1705 - acc: 0.7161\n",
      "Epoch 271/500\n",
      "3072/3072 [==============================] - 0s 74us/step - loss: 0.1717 - acc: 0.7074\n",
      "Epoch 272/500\n",
      "3072/3072 [==============================] - 0s 73us/step - loss: 0.1708 - acc: 0.7201\n",
      "Epoch 273/500\n",
      "3072/3072 [==============================] - 0s 75us/step - loss: 0.1703 - acc: 0.7181\n",
      "Epoch 274/500\n",
      "3072/3072 [==============================] - 0s 75us/step - loss: 0.1698 - acc: 0.7174\n",
      "Epoch 275/500\n",
      "3072/3072 [==============================] - 0s 73us/step - loss: 0.1699 - acc: 0.7100\n",
      "Epoch 276/500\n",
      "3072/3072 [==============================] - 0s 78us/step - loss: 0.1696 - acc: 0.7109\n",
      "Epoch 277/500\n",
      "3072/3072 [==============================] - 0s 76us/step - loss: 0.1707 - acc: 0.7165\n",
      "Epoch 278/500\n",
      "3072/3072 [==============================] - 0s 73us/step - loss: 0.1700 - acc: 0.7204\n",
      "Epoch 279/500\n",
      "3072/3072 [==============================] - 0s 77us/step - loss: 0.1703 - acc: 0.7168\n",
      "Epoch 280/500\n",
      "3072/3072 [==============================] - 0s 76us/step - loss: 0.1708 - acc: 0.7116\n",
      "Epoch 281/500\n",
      "3072/3072 [==============================] - 0s 72us/step - loss: 0.1699 - acc: 0.7178\n",
      "Epoch 282/500\n",
      "3072/3072 [==============================] - 0s 77us/step - loss: 0.1700 - acc: 0.7090\n",
      "Epoch 283/500\n",
      "3072/3072 [==============================] - 0s 73us/step - loss: 0.1702 - acc: 0.7220\n",
      "Epoch 284/500\n",
      "3072/3072 [==============================] - 0s 73us/step - loss: 0.1694 - acc: 0.7122\n",
      "Epoch 285/500\n",
      "3072/3072 [==============================] - 0s 69us/step - loss: 0.1710 - acc: 0.7188\n",
      "Epoch 286/500\n",
      "3072/3072 [==============================] - 0s 74us/step - loss: 0.1672 - acc: 0.7279\n",
      "Epoch 287/500\n",
      "3072/3072 [==============================] - 0s 77us/step - loss: 0.1690 - acc: 0.7184\n",
      "Epoch 288/500\n",
      "3072/3072 [==============================] - 0s 78us/step - loss: 0.1683 - acc: 0.7246\n",
      "Epoch 289/500\n",
      "3072/3072 [==============================] - 0s 75us/step - loss: 0.1687 - acc: 0.7220\n",
      "Epoch 290/500\n",
      "3072/3072 [==============================] - 0s 78us/step - loss: 0.1712 - acc: 0.7152\n",
      "Epoch 291/500\n",
      "3072/3072 [==============================] - 0s 76us/step - loss: 0.1686 - acc: 0.7207\n",
      "Epoch 292/500\n",
      "3072/3072 [==============================] - 0s 77us/step - loss: 0.1690 - acc: 0.7168\n",
      "Epoch 293/500\n",
      "3072/3072 [==============================] - 0s 73us/step - loss: 0.1663 - acc: 0.7194\n",
      "Epoch 294/500\n",
      "3072/3072 [==============================] - 0s 72us/step - loss: 0.1672 - acc: 0.7168\n",
      "Epoch 295/500\n",
      "3072/3072 [==============================] - 0s 75us/step - loss: 0.1676 - acc: 0.7223\n",
      "Epoch 296/500\n",
      "3072/3072 [==============================] - 0s 75us/step - loss: 0.1671 - acc: 0.7181\n",
      "Epoch 297/500\n",
      "3072/3072 [==============================] - 0s 71us/step - loss: 0.1660 - acc: 0.7295\n",
      "Epoch 298/500\n",
      "3072/3072 [==============================] - 0s 73us/step - loss: 0.1670 - acc: 0.7233\n",
      "Epoch 299/500\n",
      "3072/3072 [==============================] - 0s 70us/step - loss: 0.1672 - acc: 0.7243\n",
      "Epoch 300/500\n",
      "3072/3072 [==============================] - 0s 74us/step - loss: 0.1680 - acc: 0.7249\n",
      "Epoch 301/500\n",
      "3072/3072 [==============================] - 0s 73us/step - loss: 0.1667 - acc: 0.7243\n",
      "Epoch 302/500\n",
      "3072/3072 [==============================] - 0s 78us/step - loss: 0.1656 - acc: 0.7256\n",
      "Epoch 303/500\n",
      "3072/3072 [==============================] - 0s 70us/step - loss: 0.1676 - acc: 0.7227\n",
      "Epoch 304/500\n",
      "3072/3072 [==============================] - 0s 72us/step - loss: 0.1658 - acc: 0.7269\n",
      "Epoch 305/500\n",
      "3072/3072 [==============================] - 0s 69us/step - loss: 0.1671 - acc: 0.7227\n",
      "Epoch 306/500\n",
      "3072/3072 [==============================] - 0s 80us/step - loss: 0.1671 - acc: 0.7259\n",
      "Epoch 307/500\n",
      "3072/3072 [==============================] - 0s 78us/step - loss: 0.1648 - acc: 0.7240\n",
      "Epoch 308/500\n",
      "3072/3072 [==============================] - 0s 76us/step - loss: 0.1660 - acc: 0.7262\n",
      "Epoch 309/500\n",
      "3072/3072 [==============================] - 0s 69us/step - loss: 0.1658 - acc: 0.7233\n",
      "Epoch 310/500\n",
      "3072/3072 [==============================] - 0s 77us/step - loss: 0.1665 - acc: 0.7269\n",
      "Epoch 311/500\n",
      "3072/3072 [==============================] - 0s 74us/step - loss: 0.1658 - acc: 0.7243\n",
      "Epoch 312/500\n",
      "3072/3072 [==============================] - 0s 75us/step - loss: 0.1651 - acc: 0.7295\n",
      "Epoch 313/500\n",
      "3072/3072 [==============================] - 0s 73us/step - loss: 0.1649 - acc: 0.7314\n",
      "Epoch 314/500\n",
      "3072/3072 [==============================] - 0s 71us/step - loss: 0.1661 - acc: 0.7279\n",
      "Epoch 315/500\n",
      "3072/3072 [==============================] - 0s 76us/step - loss: 0.1674 - acc: 0.7246\n",
      "Epoch 316/500\n",
      "3072/3072 [==============================] - 0s 75us/step - loss: 0.1650 - acc: 0.7298\n",
      "Epoch 317/500\n",
      "3072/3072 [==============================] - 0s 71us/step - loss: 0.1648 - acc: 0.7331\n",
      "Epoch 318/500\n",
      "3072/3072 [==============================] - 0s 73us/step - loss: 0.1667 - acc: 0.7233\n",
      "Epoch 319/500\n",
      "3072/3072 [==============================] - 0s 71us/step - loss: 0.1653 - acc: 0.7318\n",
      "Epoch 320/500\n",
      "3072/3072 [==============================] - 0s 82us/step - loss: 0.1661 - acc: 0.7262\n",
      "Epoch 321/500\n",
      "3072/3072 [==============================] - 0s 76us/step - loss: 0.1629 - acc: 0.7334\n",
      "Epoch 322/500\n",
      "3072/3072 [==============================] - 0s 76us/step - loss: 0.1637 - acc: 0.7334\n",
      "Epoch 323/500\n",
      "3072/3072 [==============================] - 0s 72us/step - loss: 0.1640 - acc: 0.7311\n",
      "Epoch 324/500\n",
      "3072/3072 [==============================] - 0s 78us/step - loss: 0.1626 - acc: 0.7331\n",
      "Epoch 325/500\n",
      "3072/3072 [==============================] - 0s 71us/step - loss: 0.1639 - acc: 0.7305\n",
      "Epoch 326/500\n",
      "3072/3072 [==============================] - 0s 71us/step - loss: 0.1641 - acc: 0.7318\n",
      "Epoch 327/500\n",
      "3072/3072 [==============================] - 0s 80us/step - loss: 0.1650 - acc: 0.7214\n",
      "Epoch 328/500\n",
      "3072/3072 [==============================] - 0s 72us/step - loss: 0.1676 - acc: 0.7292\n",
      "Epoch 329/500\n",
      "3072/3072 [==============================] - 0s 79us/step - loss: 0.1647 - acc: 0.7331\n",
      "Epoch 330/500\n",
      "3072/3072 [==============================] - 0s 76us/step - loss: 0.1682 - acc: 0.7240\n",
      "Epoch 331/500\n",
      "3072/3072 [==============================] - 0s 59us/step - loss: 0.1660 - acc: 0.7311\n",
      "Epoch 332/500\n",
      "3072/3072 [==============================] - 0s 53us/step - loss: 0.1667 - acc: 0.7191\n",
      "Epoch 333/500\n",
      "3072/3072 [==============================] - 0s 47us/step - loss: 0.1644 - acc: 0.7298\n",
      "Epoch 334/500\n",
      "3072/3072 [==============================] - 0s 49us/step - loss: 0.1647 - acc: 0.7318\n",
      "Epoch 335/500\n",
      "3072/3072 [==============================] - 0s 48us/step - loss: 0.1630 - acc: 0.7321\n",
      "Epoch 336/500\n",
      "3072/3072 [==============================] - 0s 47us/step - loss: 0.1636 - acc: 0.7350\n",
      "Epoch 337/500\n",
      "3072/3072 [==============================] - 0s 49us/step - loss: 0.1629 - acc: 0.7337\n",
      "Epoch 338/500\n",
      "3072/3072 [==============================] - 0s 49us/step - loss: 0.1628 - acc: 0.7298\n",
      "Epoch 339/500\n",
      "3072/3072 [==============================] - 0s 49us/step - loss: 0.1619 - acc: 0.7347\n",
      "Epoch 340/500\n",
      "3072/3072 [==============================] - 0s 49us/step - loss: 0.1629 - acc: 0.7324\n",
      "Epoch 341/500\n",
      "3072/3072 [==============================] - 0s 47us/step - loss: 0.1634 - acc: 0.7331\n",
      "Epoch 342/500\n",
      "3072/3072 [==============================] - 0s 49us/step - loss: 0.1624 - acc: 0.7344\n",
      "Epoch 343/500\n",
      "3072/3072 [==============================] - 0s 51us/step - loss: 0.1623 - acc: 0.7279\n",
      "Epoch 344/500\n",
      "3072/3072 [==============================] - 0s 50us/step - loss: 0.1616 - acc: 0.7249\n",
      "Epoch 345/500\n",
      "3072/3072 [==============================] - 0s 51us/step - loss: 0.1607 - acc: 0.7370\n",
      "Epoch 346/500\n",
      "3072/3072 [==============================] - 0s 48us/step - loss: 0.1619 - acc: 0.7318\n",
      "Epoch 347/500\n",
      "3072/3072 [==============================] - 0s 48us/step - loss: 0.1619 - acc: 0.7318\n",
      "Epoch 348/500\n",
      "3072/3072 [==============================] - 0s 70us/step - loss: 0.1620 - acc: 0.7334\n",
      "Epoch 349/500\n",
      "3072/3072 [==============================] - 0s 90us/step - loss: 0.1621 - acc: 0.7288\n",
      "Epoch 350/500\n",
      "3072/3072 [==============================] - 0s 87us/step - loss: 0.1608 - acc: 0.7386\n",
      "Epoch 351/500\n",
      "3072/3072 [==============================] - 0s 88us/step - loss: 0.1608 - acc: 0.7308\n",
      "Epoch 352/500\n",
      "3072/3072 [==============================] - 0s 86us/step - loss: 0.1612 - acc: 0.7331\n",
      "Epoch 353/500\n",
      "3072/3072 [==============================] - 0s 70us/step - loss: 0.1607 - acc: 0.7373\n",
      "Epoch 354/500\n",
      "3072/3072 [==============================] - 0s 73us/step - loss: 0.1605 - acc: 0.7327\n",
      "Epoch 355/500\n",
      "3072/3072 [==============================] - 0s 76us/step - loss: 0.1633 - acc: 0.7295\n",
      "Epoch 356/500\n",
      "3072/3072 [==============================] - 0s 71us/step - loss: 0.1604 - acc: 0.7363\n",
      "Epoch 357/500\n",
      "3072/3072 [==============================] - 0s 71us/step - loss: 0.1626 - acc: 0.7318\n",
      "Epoch 358/500\n",
      "3072/3072 [==============================] - 0s 71us/step - loss: 0.1653 - acc: 0.7282\n",
      "Epoch 359/500\n",
      "3072/3072 [==============================] - 0s 70us/step - loss: 0.1617 - acc: 0.7314\n",
      "Epoch 360/500\n",
      "3072/3072 [==============================] - 0s 72us/step - loss: 0.1602 - acc: 0.7357\n",
      "Epoch 361/500\n",
      "3072/3072 [==============================] - 0s 76us/step - loss: 0.1599 - acc: 0.7393\n",
      "Epoch 362/500\n",
      "3072/3072 [==============================] - 0s 71us/step - loss: 0.1594 - acc: 0.7441\n",
      "Epoch 363/500\n",
      "3072/3072 [==============================] - 0s 73us/step - loss: 0.1598 - acc: 0.7367\n",
      "Epoch 364/500\n",
      "3072/3072 [==============================] - 0s 66us/step - loss: 0.1624 - acc: 0.7380\n",
      "Epoch 365/500\n",
      "3072/3072 [==============================] - 0s 71us/step - loss: 0.1606 - acc: 0.7318\n",
      "Epoch 366/500\n",
      "3072/3072 [==============================] - 0s 70us/step - loss: 0.1606 - acc: 0.7367\n",
      "Epoch 367/500\n",
      "3072/3072 [==============================] - 0s 70us/step - loss: 0.1592 - acc: 0.7327\n",
      "Epoch 368/500\n",
      "3072/3072 [==============================] - 0s 73us/step - loss: 0.1606 - acc: 0.7357\n",
      "Epoch 369/500\n",
      "3072/3072 [==============================] - 0s 74us/step - loss: 0.1604 - acc: 0.7402\n",
      "Epoch 370/500\n",
      "3072/3072 [==============================] - 0s 70us/step - loss: 0.1627 - acc: 0.7347\n",
      "Epoch 371/500\n",
      "3072/3072 [==============================] - 0s 66us/step - loss: 0.1582 - acc: 0.7415\n",
      "Epoch 372/500\n",
      "3072/3072 [==============================] - 0s 70us/step - loss: 0.1595 - acc: 0.7327\n",
      "Epoch 373/500\n",
      "3072/3072 [==============================] - 0s 72us/step - loss: 0.1589 - acc: 0.7422\n",
      "Epoch 374/500\n",
      "3072/3072 [==============================] - 0s 81us/step - loss: 0.1575 - acc: 0.7406\n",
      "Epoch 375/500\n",
      "3072/3072 [==============================] - 0s 75us/step - loss: 0.1568 - acc: 0.7484\n",
      "Epoch 376/500\n",
      "3072/3072 [==============================] - 0s 79us/step - loss: 0.1589 - acc: 0.7445\n",
      "Epoch 377/500\n",
      "3072/3072 [==============================] - 0s 71us/step - loss: 0.1561 - acc: 0.7445\n",
      "Epoch 378/500\n",
      "3072/3072 [==============================] - 0s 70us/step - loss: 0.1602 - acc: 0.7324\n",
      "Epoch 379/500\n",
      "3072/3072 [==============================] - 0s 74us/step - loss: 0.1591 - acc: 0.7360\n",
      "Epoch 380/500\n",
      "3072/3072 [==============================] - 0s 72us/step - loss: 0.1592 - acc: 0.7354\n",
      "Epoch 381/500\n",
      "3072/3072 [==============================] - 0s 70us/step - loss: 0.1587 - acc: 0.7344\n",
      "Epoch 382/500\n",
      "3072/3072 [==============================] - 0s 73us/step - loss: 0.1586 - acc: 0.7367\n",
      "Epoch 383/500\n",
      "3072/3072 [==============================] - 0s 70us/step - loss: 0.1586 - acc: 0.7386\n",
      "Epoch 384/500\n",
      "3072/3072 [==============================] - 0s 70us/step - loss: 0.1565 - acc: 0.7474\n",
      "Epoch 385/500\n",
      "3072/3072 [==============================] - 0s 70us/step - loss: 0.1587 - acc: 0.7419\n",
      "Epoch 386/500\n",
      "3072/3072 [==============================] - 0s 72us/step - loss: 0.1572 - acc: 0.7389\n",
      "Epoch 387/500\n",
      "3072/3072 [==============================] - 0s 69us/step - loss: 0.1578 - acc: 0.7386\n",
      "Epoch 388/500\n",
      "3072/3072 [==============================] - 0s 75us/step - loss: 0.1575 - acc: 0.7412\n",
      "Epoch 389/500\n",
      "3072/3072 [==============================] - 0s 73us/step - loss: 0.1574 - acc: 0.7399\n",
      "Epoch 390/500\n",
      "3072/3072 [==============================] - 0s 70us/step - loss: 0.1603 - acc: 0.7415\n",
      "Epoch 391/500\n",
      "3072/3072 [==============================] - 0s 75us/step - loss: 0.1592 - acc: 0.7402\n",
      "Epoch 392/500\n",
      "3072/3072 [==============================] - 0s 76us/step - loss: 0.1573 - acc: 0.7409\n",
      "Epoch 393/500\n",
      "3072/3072 [==============================] - 0s 87us/step - loss: 0.1574 - acc: 0.7435\n",
      "Epoch 394/500\n",
      "3072/3072 [==============================] - 0s 74us/step - loss: 0.1582 - acc: 0.7399\n",
      "Epoch 395/500\n",
      "3072/3072 [==============================] - 0s 67us/step - loss: 0.1554 - acc: 0.7412\n",
      "Epoch 396/500\n",
      "3072/3072 [==============================] - 0s 68us/step - loss: 0.1561 - acc: 0.7383\n",
      "Epoch 397/500\n",
      "3072/3072 [==============================] - 0s 78us/step - loss: 0.1554 - acc: 0.7480\n",
      "Epoch 398/500\n",
      "3072/3072 [==============================] - 0s 68us/step - loss: 0.1593 - acc: 0.7363\n",
      "Epoch 399/500\n",
      "3072/3072 [==============================] - 0s 72us/step - loss: 0.1605 - acc: 0.7432\n",
      "Epoch 400/500\n",
      "3072/3072 [==============================] - 0s 69us/step - loss: 0.1606 - acc: 0.7337\n",
      "Epoch 401/500\n",
      "3072/3072 [==============================] - 0s 74us/step - loss: 0.1610 - acc: 0.7327\n",
      "Epoch 402/500\n",
      "3072/3072 [==============================] - 0s 72us/step - loss: 0.1566 - acc: 0.7428\n",
      "Epoch 403/500\n",
      "3072/3072 [==============================] - 0s 75us/step - loss: 0.1573 - acc: 0.7393\n",
      "Epoch 404/500\n",
      "3072/3072 [==============================] - 0s 100us/step - loss: 0.1577 - acc: 0.7350\n",
      "Epoch 405/500\n",
      "3072/3072 [==============================] - 0s 99us/step - loss: 0.1556 - acc: 0.7389\n",
      "Epoch 406/500\n",
      "3072/3072 [==============================] - 0s 74us/step - loss: 0.1558 - acc: 0.7422\n",
      "Epoch 407/500\n",
      "3072/3072 [==============================] - 0s 72us/step - loss: 0.1567 - acc: 0.7435\n",
      "Epoch 408/500\n",
      "3072/3072 [==============================] - 0s 128us/step - loss: 0.1567 - acc: 0.7419\n",
      "Epoch 409/500\n",
      "3072/3072 [==============================] - 0s 88us/step - loss: 0.1558 - acc: 0.7454\n",
      "Epoch 410/500\n",
      "3072/3072 [==============================] - 0s 72us/step - loss: 0.1548 - acc: 0.7448\n",
      "Epoch 411/500\n",
      "3072/3072 [==============================] - 0s 71us/step - loss: 0.1551 - acc: 0.7454\n",
      "Epoch 412/500\n",
      "3072/3072 [==============================] - 0s 126us/step - loss: 0.1548 - acc: 0.7451\n",
      "Epoch 413/500\n",
      "3072/3072 [==============================] - 0s 75us/step - loss: 0.1565 - acc: 0.7370\n",
      "Epoch 414/500\n",
      "3072/3072 [==============================] - 0s 94us/step - loss: 0.1561 - acc: 0.7386\n",
      "Epoch 415/500\n",
      "3072/3072 [==============================] - 0s 96us/step - loss: 0.1534 - acc: 0.7510\n",
      "Epoch 416/500\n",
      "3072/3072 [==============================] - 0s 91us/step - loss: 0.1560 - acc: 0.7393\n",
      "Epoch 417/500\n",
      "3072/3072 [==============================] - 0s 102us/step - loss: 0.1568 - acc: 0.7451\n",
      "Epoch 418/500\n",
      "3072/3072 [==============================] - 0s 75us/step - loss: 0.1560 - acc: 0.7448\n",
      "Epoch 419/500\n",
      "3072/3072 [==============================] - 0s 107us/step - loss: 0.1545 - acc: 0.7484\n",
      "Epoch 420/500\n",
      "3072/3072 [==============================] - 0s 95us/step - loss: 0.1551 - acc: 0.7435\n",
      "Epoch 421/500\n",
      "3072/3072 [==============================] - 0s 89us/step - loss: 0.1551 - acc: 0.7464\n",
      "Epoch 422/500\n",
      "3072/3072 [==============================] - 0s 92us/step - loss: 0.1545 - acc: 0.7428\n",
      "Epoch 423/500\n",
      "3072/3072 [==============================] - 0s 72us/step - loss: 0.1529 - acc: 0.7451\n",
      "Epoch 424/500\n",
      "3072/3072 [==============================] - 0s 111us/step - loss: 0.1547 - acc: 0.7454\n",
      "Epoch 425/500\n",
      "3072/3072 [==============================] - 0s 89us/step - loss: 0.1556 - acc: 0.7451\n",
      "Epoch 426/500\n",
      "3072/3072 [==============================] - 0s 118us/step - loss: 0.1550 - acc: 0.7425\n",
      "Epoch 427/500\n",
      "3072/3072 [==============================] - 0s 86us/step - loss: 0.1543 - acc: 0.7458\n",
      "Epoch 428/500\n",
      "3072/3072 [==============================] - 0s 116us/step - loss: 0.1536 - acc: 0.7490\n",
      "Epoch 429/500\n",
      "3072/3072 [==============================] - 0s 75us/step - loss: 0.1540 - acc: 0.7461\n",
      "Epoch 430/500\n",
      "3072/3072 [==============================] - 0s 84us/step - loss: 0.1538 - acc: 0.7461\n",
      "Epoch 431/500\n",
      "3072/3072 [==============================] - 0s 96us/step - loss: 0.1534 - acc: 0.7487\n",
      "Epoch 432/500\n",
      "3072/3072 [==============================] - 0s 55us/step - loss: 0.1537 - acc: 0.7536\n",
      "Epoch 433/500\n",
      "3072/3072 [==============================] - 0s 58us/step - loss: 0.1546 - acc: 0.7454\n",
      "Epoch 434/500\n",
      "3072/3072 [==============================] - 0s 64us/step - loss: 0.1544 - acc: 0.7487\n",
      "Epoch 435/500\n",
      "3072/3072 [==============================] - 0s 98us/step - loss: 0.1543 - acc: 0.7445\n",
      "Epoch 436/500\n",
      "3072/3072 [==============================] - 0s 57us/step - loss: 0.1544 - acc: 0.7471\n",
      "Epoch 437/500\n",
      "3072/3072 [==============================] - 0s 55us/step - loss: 0.1522 - acc: 0.7471\n",
      "Epoch 438/500\n",
      "3072/3072 [==============================] - 0s 54us/step - loss: 0.1535 - acc: 0.7435\n",
      "Epoch 439/500\n",
      "3072/3072 [==============================] - 0s 54us/step - loss: 0.1525 - acc: 0.7480\n",
      "Epoch 440/500\n",
      "3072/3072 [==============================] - 0s 55us/step - loss: 0.1522 - acc: 0.7484\n",
      "Epoch 441/500\n",
      "3072/3072 [==============================] - 0s 54us/step - loss: 0.1528 - acc: 0.7533\n",
      "Epoch 442/500\n",
      "3072/3072 [==============================] - 0s 53us/step - loss: 0.1526 - acc: 0.7438\n",
      "Epoch 443/500\n",
      "3072/3072 [==============================] - 0s 55us/step - loss: 0.1539 - acc: 0.7464\n",
      "Epoch 444/500\n",
      "3072/3072 [==============================] - 0s 57us/step - loss: 0.1563 - acc: 0.7480\n",
      "Epoch 445/500\n",
      "3072/3072 [==============================] - 0s 55us/step - loss: 0.1510 - acc: 0.7533\n",
      "Epoch 446/500\n",
      "3072/3072 [==============================] - 0s 55us/step - loss: 0.1516 - acc: 0.7477\n",
      "Epoch 447/500\n",
      "3072/3072 [==============================] - 0s 54us/step - loss: 0.1529 - acc: 0.7516\n",
      "Epoch 448/500\n",
      "3072/3072 [==============================] - 0s 54us/step - loss: 0.1520 - acc: 0.7461\n",
      "Epoch 449/500\n",
      "3072/3072 [==============================] - 0s 81us/step - loss: 0.1558 - acc: 0.7373\n",
      "Epoch 450/500\n",
      "3072/3072 [==============================] - 0s 83us/step - loss: 0.1541 - acc: 0.7471\n",
      "Epoch 451/500\n",
      "3072/3072 [==============================] - 0s 58us/step - loss: 0.1537 - acc: 0.7480\n",
      "Epoch 452/500\n",
      "3072/3072 [==============================] - 0s 60us/step - loss: 0.1524 - acc: 0.7493\n",
      "Epoch 453/500\n",
      "3072/3072 [==============================] - 0s 53us/step - loss: 0.1543 - acc: 0.7461\n",
      "Epoch 454/500\n",
      "3072/3072 [==============================] - 0s 59us/step - loss: 0.1537 - acc: 0.7474\n",
      "Epoch 455/500\n",
      "3072/3072 [==============================] - 0s 55us/step - loss: 0.1519 - acc: 0.7549\n",
      "Epoch 456/500\n",
      "3072/3072 [==============================] - 0s 55us/step - loss: 0.1518 - acc: 0.7510\n",
      "Epoch 457/500\n",
      "3072/3072 [==============================] - 0s 73us/step - loss: 0.1497 - acc: 0.7562\n",
      "Epoch 458/500\n",
      "3072/3072 [==============================] - 0s 57us/step - loss: 0.1499 - acc: 0.7526\n",
      "Epoch 459/500\n",
      "3072/3072 [==============================] - 0s 54us/step - loss: 0.1495 - acc: 0.7565\n",
      "Epoch 460/500\n",
      "3072/3072 [==============================] - 0s 57us/step - loss: 0.1544 - acc: 0.7464\n",
      "Epoch 461/500\n",
      "3072/3072 [==============================] - 0s 61us/step - loss: 0.1528 - acc: 0.7477\n",
      "Epoch 462/500\n",
      "3072/3072 [==============================] - 0s 57us/step - loss: 0.1507 - acc: 0.7523\n",
      "Epoch 463/500\n",
      "3072/3072 [==============================] - 0s 66us/step - loss: 0.1517 - acc: 0.7510\n",
      "Epoch 464/500\n",
      "3072/3072 [==============================] - 0s 58us/step - loss: 0.1515 - acc: 0.7510\n",
      "Epoch 465/500\n",
      "3072/3072 [==============================] - 0s 59us/step - loss: 0.1497 - acc: 0.7513\n",
      "Epoch 466/500\n",
      "3072/3072 [==============================] - 0s 73us/step - loss: 0.1551 - acc: 0.7497\n",
      "Epoch 467/500\n",
      "3072/3072 [==============================] - 0s 106us/step - loss: 0.1513 - acc: 0.7484\n",
      "Epoch 468/500\n",
      "3072/3072 [==============================] - 0s 64us/step - loss: 0.1506 - acc: 0.7500\n",
      "Epoch 469/500\n",
      "3072/3072 [==============================] - 0s 62us/step - loss: 0.1518 - acc: 0.7480\n",
      "Epoch 470/500\n",
      "3072/3072 [==============================] - 0s 73us/step - loss: 0.1531 - acc: 0.7471\n",
      "Epoch 471/500\n",
      "3072/3072 [==============================] - 0s 109us/step - loss: 0.1533 - acc: 0.7487\n",
      "Epoch 472/500\n",
      "3072/3072 [==============================] - 0s 70us/step - loss: 0.1519 - acc: 0.7562\n",
      "Epoch 473/500\n",
      "3072/3072 [==============================] - 0s 71us/step - loss: 0.1496 - acc: 0.7523\n",
      "Epoch 474/500\n",
      "3072/3072 [==============================] - 0s 105us/step - loss: 0.1499 - acc: 0.7542\n",
      "Epoch 475/500\n",
      "3072/3072 [==============================] - 0s 96us/step - loss: 0.1493 - acc: 0.7552\n",
      "Epoch 476/500\n",
      "3072/3072 [==============================] - 0s 70us/step - loss: 0.1490 - acc: 0.7552\n",
      "Epoch 477/500\n",
      "3072/3072 [==============================] - 0s 104us/step - loss: 0.1495 - acc: 0.7568\n",
      "Epoch 478/500\n",
      "3072/3072 [==============================] - 0s 98us/step - loss: 0.1535 - acc: 0.7467\n",
      "Epoch 479/500\n",
      "3072/3072 [==============================] - 0s 80us/step - loss: 0.1522 - acc: 0.7529\n",
      "Epoch 480/500\n",
      "3072/3072 [==============================] - 0s 109us/step - loss: 0.1510 - acc: 0.7575\n",
      "Epoch 481/500\n",
      "3072/3072 [==============================] - 0s 95us/step - loss: 0.1490 - acc: 0.7523\n",
      "Epoch 482/500\n",
      "3072/3072 [==============================] - 0s 70us/step - loss: 0.1502 - acc: 0.7549\n",
      "Epoch 483/500\n",
      "3072/3072 [==============================] - 0s 75us/step - loss: 0.1489 - acc: 0.7513\n",
      "Epoch 484/500\n",
      "3072/3072 [==============================] - 0s 124us/step - loss: 0.1486 - acc: 0.7568\n",
      "Epoch 485/500\n",
      "3072/3072 [==============================] - 0s 78us/step - loss: 0.1521 - acc: 0.7516\n",
      "Epoch 486/500\n",
      "3072/3072 [==============================] - 0s 69us/step - loss: 0.1516 - acc: 0.7484\n",
      "Epoch 487/500\n",
      "3072/3072 [==============================] - 0s 94us/step - loss: 0.1497 - acc: 0.7503\n",
      "Epoch 488/500\n",
      "3072/3072 [==============================] - 0s 93us/step - loss: 0.1503 - acc: 0.7435\n",
      "Epoch 489/500\n",
      "3072/3072 [==============================] - 0s 73us/step - loss: 0.1481 - acc: 0.7581\n",
      "Epoch 490/500\n",
      "3072/3072 [==============================] - 0s 72us/step - loss: 0.1474 - acc: 0.7614\n",
      "Epoch 491/500\n",
      "3072/3072 [==============================] - 0s 112us/step - loss: 0.1493 - acc: 0.7507\n",
      "Epoch 492/500\n",
      "3072/3072 [==============================] - 0s 87us/step - loss: 0.1493 - acc: 0.7568: 0s - loss: 0.1463 - acc: 0\n",
      "Epoch 493/500\n",
      "3072/3072 [==============================] - 0s 70us/step - loss: 0.1504 - acc: 0.7568\n",
      "Epoch 494/500\n",
      "3072/3072 [==============================] - 0s 109us/step - loss: 0.1495 - acc: 0.7516\n",
      "Epoch 495/500\n",
      "3072/3072 [==============================] - 0s 95us/step - loss: 0.1495 - acc: 0.7490\n",
      "Epoch 496/500\n",
      "3072/3072 [==============================] - 0s 73us/step - loss: 0.1492 - acc: 0.7529\n",
      "Epoch 497/500\n",
      "3072/3072 [==============================] - 0s 78us/step - loss: 0.1513 - acc: 0.7555\n",
      "Epoch 498/500\n",
      "3072/3072 [==============================] - 0s 118us/step - loss: 0.1492 - acc: 0.7539\n",
      "Epoch 499/500\n",
      "3072/3072 [==============================] - 0s 83us/step - loss: 0.1499 - acc: 0.7520\n",
      "Epoch 500/500\n",
      "3072/3072 [==============================] - 0s 76us/step - loss: 0.1539 - acc: 0.7441\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fb8e14500d0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit final model and evaluate results\n",
    "\n",
    "model.fit(X_train, y_train, \n",
    "          batch_size=20, \n",
    "          epochs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model for future use\n",
    "\n",
    "model.save('./models/goog_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Predict the Movement of Stock:\n",
    "\n",
    "Now that the neural network has been compiled, use the predict() method for making the prediction. \n",
    "Pass X_test as its argument and store the result in a variable named ypred. \n",
    "Then convert ypred to store binary values by storing the condition ypred > 5. \n",
    "Now, the variable y_pred stores either True or False depending on whether the \n",
    "predicted value was greater or less than 0.5.\n",
    "'''\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = (y_pred > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Next, create a new column in the dataframe dataset with the column header ‘ypred’ \n",
    "and store NaN values in the column. \n",
    "Then store the values of ypred into this new column, starting from the rows of the test dataset. \n",
    "This is done by slicing the dataframe using the iloc method as shown in the code below. \n",
    "Then drop all the NaN values from the dataset and store them in a new dataframe named trade_dataset.\n",
    "'''\n",
    "\n",
    "dataset['y_pred'] = np.NaN\n",
    "dataset.iloc[(len(dataset) - len(y_pred)):,-1:] = y_pred\n",
    "trade_dataset = dataset.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing Strategy Returns\n",
    "\n",
    "trade_dataset['Tomorrows Returns'] = 0.\n",
    "trade_dataset['Tomorrows Returns'] = np.log(trade_dataset['Close']/trade_dataset['Close'].shift(1))\n",
    "trade_dataset['Tomorrows Returns'] = trade_dataset['Tomorrows Returns'].shift(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "trade_dataset['Strategy Returns'] = 0.\n",
    "trade_dataset['Strategy Returns'] = np.where(trade_dataset['y_pred'] == True, trade_dataset['Tomorrows Returns'], - trade_dataset['Tomorrows Returns'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "trade_dataset['Cumulative Market Returns'] = np.cumsum(trade_dataset['Tomorrows Returns'])\n",
    "trade_dataset['Cumulative Strategy Returns'] = np.cumsum(trade_dataset['Strategy Returns'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Graph the Results\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(trade_dataset['Cumulative Market Returns'], color='r', label='Market Returns')\n",
    "plt.plot(trade_dataset['Cumulative Strategy Returns'], color='g', label='Strategy Returns')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consider: Gather more data points (+100,000) for a more accurate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit",
   "language": "python",
   "name": "python37464bit297b18a1a3e542689b13a325bcf6fffe"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
